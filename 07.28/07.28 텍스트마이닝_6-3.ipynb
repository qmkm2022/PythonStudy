{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) LDA(Latent Dirichlet Allocation, 잠재 디리클레 할당)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주제별 단어수 분포를 바탕으로 주어진 문서에서 발견된 단어수 분포를 분석하여 해당 문서의 주제들을 예측하는 기법\n",
    "\n",
    "교환성 가정 : 단어들의 순서는 상관하지 않고 단어들의 유무만이 중요하다는 가정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "# 영어 단어의 어근만 추출\n",
    "stm = PorterStemmer()\n",
    "# 영어 단어의 불용어 집합\n",
    "stopwords = set(stopwords.words('english'))\n",
    "# 특수문자를 제거하기 위한 정규식\n",
    "# 첫글자는 알파벳으로 시작하고 그 뒤에 영문자 대소문자, 숫자, -,_, .만 허용\n",
    "# * 0회 이상 매칭, + 1회 이상 매칭\n",
    "pattern = re.compile('[a-zA-Z][-_a-zA-Z0-9.]*')\n",
    "\n",
    "# 문장을 단어 단위로 분리하고 불용어 및 특수문자 제거 후 어근만 추출하여 list로 반환\n",
    "def tokenize(sentence):\n",
    "    def stem(w):\n",
    "        try: return stm.stem(w)\n",
    "        except: return w\n",
    "    # 소문자로 바꾼 후 단어 구분, 불용어 제거, 패턴에 맞는 단어만 선택\n",
    "    return [stem(w) for w in word_tokenize(sentence.lower())\n",
    "            if w not in stopwords and pattern.match(w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/text/trumph.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m model \u001b[39m=\u001b[39m tp\u001b[39m.\u001b[39mLDAModel(k\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, min_cf\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[39m# 파일에서 한 줄씩 읽어와서 model에 추가\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfor\u001b[39;00m i, line \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39m../data/text/trumph.txt\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mms949\u001b[39;49m\u001b[39m'\u001b[39;49m)):\n\u001b[0;32m     11\u001b[0m     model\u001b[39m.\u001b[39madd_doc(tokenize(line)) \u001b[39m# 공백 기준으로 단어를 나누어 model에 추가\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m# train(0) : 0회 학습, model의 num_words, num_vocabs 값을 확인하기 위해\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m#            실제로 학습은 하지 않고 학습 준비만 하는 상태\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/text/trumph.txt'"
     ]
    }
   ],
   "source": [
    "import tomotopy as tp\n",
    "\n",
    "# LDAModel 생성\n",
    "# LDA(Latent Dirichlet allocation, 잠재 디리클레 할당)\n",
    "#   주어진 문서에 대하여 각 문서에 어떤 주제들이 존재하는지를 서술하는 확률적 토픽 모델 기법\n",
    "# 토픽의 개수(k) 20개, 5회 미만 등장한 단어들은 제거\n",
    "model = tp.LDAModel(k=20, min_cf=5)\n",
    "\n",
    "# 파일에서 한 줄씩 읽어와서 model에 추가\n",
    "for i, line in enumerate(open('../data/text/trumph.txt', encoding='ms949')):\n",
    "    model.add_doc(tokenize(line)) # 공백 기준으로 단어를 나누어 model에 추가\n",
    "\n",
    "# train(0) : 0회 학습, model의 num_words, num_vocabs 값을 확인하기 위해\n",
    "#            실제로 학습은 하지 않고 학습 준비만 하는 상태\n",
    "model.train(0)\n",
    "print('Total docs:', len(model.docs))\n",
    "print('Total words:', model.num_words)\n",
    "print(list(model.vocabs)[:50]) # 단어 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200회 학습\n",
    "model.train(200)\n",
    "# 학습된 토픽들을 출력\n",
    "for i in range(model.k):\n",
    "    # 0~19번 토픽별 상위 단어 10개 추출\n",
    "    res = model.get_topic_words(i, top_n=10)\n",
    "    print(f'Topic #{i}', end='\\t')\n",
    "    print(', '.join(w for w, p in res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "emma_raw=nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
    "\n",
    "tokenize(emma_raw)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tp.LDAModel(k=5, min_cf=5) # 토픽모델링 함수\n",
    "model.add_doc(tokenize(emma_raw)) # 단어별로 나누고 모형에 추가\n",
    "model.train(0) # 0 학습횟수, 아직 학습이 시작된 상태가 아님\n",
    "print('Total docs:',len(model.docs))\n",
    "print('Total words:',model.num_words)\n",
    "print(list(model.vocabs)[:100]) # 단어 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(100) # 100회 학습\n",
    "for i in range(model.k):\n",
    "    res=model.get_topic_words(i,top_n=2) # 토픽의 상위단어 2개\n",
    "    print(res)\n",
    "    print(f'Topic #{i}',end='\\t')\n",
    "    print(', '.join(w for w,p in res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "알고리즘에 의해 관련성 있는 단어들끼리 묶은 토픽들\n",
    "\n",
    "토픽 세부 내용을 보고 어떤 주제인지 확인하는 작업이 필요함\n",
    "\n",
    "첫번째 토픽의 중심단어 could,know\n",
    "\n",
    "could는 6.1%, know는 2.8% 등장\n",
    "\n",
    "[('could', 0.0612008310854435), ('know', 0.028761839494109154)]\n",
    "\n",
    "Topic #0    could, know\n",
    "\n",
    "두번째 토픽의 중심단어는 mrs.(4.6%) miss(4.2%)\n",
    "\n",
    "[('mrs.', 0.04658449441194534), ('miss', 0.04260953143239021)]\n",
    "\n",
    "Topic #1    mrs., miss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
