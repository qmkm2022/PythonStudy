{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 잠재의미분석(LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특이값 분해(Singular Value Decomposition, SVD)\n",
    "\n",
    "    차원축소 방법 중 하나\n",
    "\n",
    "m x n 행렬을   m x m 직교행렬, n x n 직교행렬, m x n 직사각 대각행렬  3개의 곱으로 분해하는 작업\n",
    "\n",
    "행렬 연산을 통해 데이터의 차원을 축소하고 중요한 특징들을 추출하는 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups  \n",
    "# 시간이 오래 걸림\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))  \n",
    "documents = dataset.data\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well i'm not sure about the story nad it did seem biased. What\n",
      "I disagree with is your statement that the U.S. Media is out to\n",
      "ruin Israels reputation. That is rediculous. The U.S. media is\n",
      "the most pro-israeli media in the world. Having lived in Europe\n",
      "I realize that incidences such as the one described in the\n",
      "letter have occured. The U.S. media as a whole seem to try to\n",
      "ignore them. The U.S. is subsidizing Israels existance and the\n",
      "Europeans are not (at least not to the same degree). So I think\n",
      "that might be a reason they report more clearly on the\n",
      "atrocities.\n",
      "\tWhat is a shame is that in Austria, daily reports of\n",
      "the inhuman acts commited by Israeli soldiers and the blessing\n",
      "received from the Government makes some of the Holocaust guilt\n",
      "go away. After all, look how the Jews are treating other races\n",
      "when they got power. It is unfortunate.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.target_names) # 뉴스 카테고리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well sure about story seem biased. what disagree with your statement that u.s. media ruin israels reputation. that rediculous. u.s. media most pro-israeli media world. having lived europe realize that incidences such described letter have occured. u.s. media whole seem ignore them. u.s. subsidizing israels existance europeans least same degree). think that might reason they report more clearly atrocities. what shame that austria, daily reports inhuman acts commited israeli soldiers blessing received from government makes some holocaust guilt away. after all, look jews treating other races when they power. unfortunate.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "# 알파벳 이외의 문자 제거\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "# 길이가 3이하인 단어 제거\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(\n",
    "    lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "# 소문자 변환\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())\n",
    "news_df['clean_doc'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['well', 'sure', 'story', 'seem', 'biased.', 'disagree', 'statement', 'u.s.', 'media', 'ruin', 'israels', 'reputation.', 'rediculous.', 'u.s.', 'media', 'pro-israeli', 'media', 'world.', 'lived', 'europe', 'realize', 'incidences', 'described', 'letter', 'occured.', 'u.s.', 'media', 'whole', 'seem', 'ignore', 'them.', 'u.s.', 'subsidizing', 'israels', 'existance', 'europeans', 'least', 'degree).', 'think', 'might', 'reason', 'report', 'clearly', 'atrocities.', 'shame', 'austria,', 'daily', 'reports', 'inhuman', 'acts', 'commited', 'israeli', 'soldiers', 'blessing', 'received', 'government', 'makes', 'holocaust', 'guilt', 'away.', 'all,', 'look', 'jews', 'treating', 'races', 'power.', 'unfortunate.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# 토큰화\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
    "# 불용어 제거\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "print(tokenized_doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well sure story seem biased. disagree statement u.s. media ruin israels reputation. rediculous. u.s. media pro-israeli media world. lived europe realize incidences described letter occured. u.s. media whole seem ignore them. u.s. subsidizing israels existance europeans least degree). think might reason report clearly atrocities. shame austria, daily reports inhuman acts commited israeli soldiers blessing received government makes holocaust guilt away. all, look jews treating races power. unfortunate.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf-idf 행렬을 만들기 위해 다시 역토큰화\n",
    "\n",
    "detokenized_doc = []\n",
    "for i in range(len(news_df)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "news_df['clean_doc'] = detokenized_doc\n",
    "\n",
    "news_df['clean_doc'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 상위 1000개의 단어만 처리\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000)\n",
    "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
    "X.shape # TF-IDF 행렬의 크기 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "# 행렬 특이값 분해, 11314개의 행을 20개로 축소, n_components 토픽수\n",
    "svd_model = TruncatedSVD(n_components=20)\n",
    "svd_model.fit(X)\n",
    "len(svd_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 토픽수 x 단어수\n",
    "np.shape(svd_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02682194,  0.02779475,  0.00507776, ...,  0.04554116,\n",
       "         0.01386712,  0.01758137],\n",
       "       [ 0.03501249, -0.0150116 ,  0.00534304, ..., -0.02162846,\n",
       "        -0.00912255, -0.01760022],\n",
       "       [ 0.06487863,  0.0286971 ,  0.00452922, ..., -0.00207655,\n",
       "         0.02335056,  0.02043433],\n",
       "       ...,\n",
       "       [ 0.08024134,  0.03438316,  0.00154817, ...,  0.01804123,\n",
       "        -0.003448  ,  0.0050334 ],\n",
       "       [ 0.03872506, -0.02064543, -0.00493834, ..., -0.03772561,\n",
       "        -0.00945247, -0.0099365 ],\n",
       "       [-0.07219908, -0.03848623,  0.00044259, ...,  0.01463146,\n",
       "        -0.007438  , -0.0035805 ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('like', 0.2085), ('know', 0.19656), ('people', 0.1912), ('think', 0.17523), ('good', 0.14902)]\n",
      "Topic 2: [('thanks', 0.31342), ('windows', 0.27941), ('card', 0.17295), ('drive', 0.16146), ('mail', 0.14496)]\n",
      "Topic 3: [('game', 0.36766), ('team', 0.31145), ('year', 0.28411), ('games', 0.2304), ('season', 0.16978)]\n",
      "Topic 4: [('edu', 0.50329), ('thanks', 0.25183), ('mail', 0.17635), ('email', 0.11264), ('com', 0.11197)]\n",
      "Topic 5: [('edu', 0.49887), ('drive', 0.253), ('sale', 0.10969), ('com', 0.10916), ('soon', 0.09206)]\n",
      "Topic 6: [('drive', 0.39704), ('thanks', 0.3472), ('know', 0.27989), ('scsi', 0.1382), ('mail', 0.11401)]\n",
      "Topic 7: [('chip', 0.22214), ('government', 0.20243), ('like', 0.16519), ('encryption', 0.15076), ('clipper', 0.14962)]\n",
      "Topic 8: [('like', 0.65106), ('edu', 0.3098), ('know', 0.13353), ('think', 0.12132), ('bike', 0.1212)]\n",
      "Topic 9: [('card', 0.3243), ('good', 0.27584), ('sale', 0.16749), ('00', 0.14953), ('video', 0.14545)]\n",
      "Topic 10: [('card', 0.48524), ('people', 0.31023), ('know', 0.26672), ('video', 0.22267), ('edu', 0.16195)]\n",
      "Topic 11: [('like', 0.53231), ('people', 0.3325), ('game', 0.13896), ('windows', 0.12596), ('thanks', 0.09366)]\n",
      "Topic 12: [('think', 0.70525), ('thanks', 0.18902), ('people', 0.14237), ('mail', 0.12901), ('good', 0.10981)]\n",
      "Topic 13: [('chip', 0.21251), ('jesus', 0.18337), ('know', 0.17911), ('com', 0.17014), ('game', 0.14483)]\n",
      "Topic 14: [('good', 0.39626), ('people', 0.25173), ('windows', 0.24715), ('know', 0.234), ('00', 0.17665)]\n",
      "Topic 15: [('think', 0.33346), ('know', 0.31724), ('space', 0.30198), ('00', 0.18091), ('israel', 0.11836)]\n",
      "Topic 16: [('com', 0.38302), ('people', 0.38123), ('ve', 0.28653), ('windows', 0.12272), ('list', 0.11146)]\n",
      "Topic 17: [('space', 0.42913), ('people', 0.29879), ('time', 0.17573), ('nasa', 0.14581), ('ve', 0.11097)]\n",
      "Topic 18: [('ve', 0.48913), ('file', 0.16593), ('seen', 0.15669), ('bike', 0.15135), ('work', 0.12747)]\n",
      "Topic 19: [('time', 0.34398), ('right', 0.29115), ('file', 0.20769), ('sale', 0.14506), ('offer', 0.13906)]\n",
      "Topic 20: [('right', 0.30877), ('windows', 0.23874), ('bike', 0.19847), ('card', 0.17863), ('space', 0.17386)]\n"
     ]
    }
   ],
   "source": [
    "# 단어 집합, 1000개의 단어\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 20개의 뉴스그룹별로 추출한 토픽 리스트 출력\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1),\n",
    "              [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "        \n",
    "get_topics(svd_model.components_,terms)\n",
    "# 각 토픽의 핵심 키워드 추출"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
